# CC_2026-01-08

DATE: 2026-01-08
TYPE: Cognitive Checkpoint
MODE: SNAPSHOT
SOURCE: Human + Local LLM / Large LLM mixed operation

## 1. 現在の到達点（要約）

ローカルLLMを実運用（CC生成・要約・未確定点抽出）まで試した結果、
以下が明確になった。

- ローカルLLMでも CC は十分に機能する
- CCを前提にした「要約」「未確定点抽出」「次の問い提示」は実用レベル
- 一方で、**問いの立ち上げ段階**では負荷が高く、思考が止まりやすい
- これは能力不足ではなく、ローカルLLMの設計思想と役割の問題

結果として、
「ローカルLLM＋CCで扱うべき問い」と
「大規模LLMで扱うべき問い」の性質差が可視化された。

---

## 2. ローカルLLMの適正役割

ローカルLLMは以下に適している。

- 長期間にわたって育てる問い
- 状態遷移そのものが価値になる思考
- CCの整理・圧縮・再提示
- 未確定点・矛盾候補・論点抽出
- 変換・要約・定型処理

逆に不向き。

- 問いの初期生成
- 情緒的・逸脱的な創作探索
- 倫理的に揺らぎのある小説支援
- 雑談ベースの思考立ち上げ

→ **ローカルLLMは「第2段・第3段の思考装置」**

---

## 3. 大規模LLMの適正役割

大規模LLMは以下に適している。

- 問いの立ち上げ
- 曖昧な相談の受け止め
- 思考の拡散・揺らぎ
- 小説・官能・価値転倒を含む探索
- 「間違ってもよい」前提の思考

→ **大規模LLMは「思考の起爆装置」**

---

## 4. 現在の結論（暫定）

- 主運用は **大規模LLM**
- ローカルLLMは **補助・整理・保存側**
- CCは両者を接続する「共有メモリ」

ローカルLLMは研究・実装検討は続けるが、
創作や探索の主戦場にはしない。

---

## 5. 今後の方針

### 短期（当面）
- CC運用を継続
- 機能強化より **運用ログの蓄積**を優先
- CCを「作る／作らない日」両方を試す
- CCの粒度・保存密度を体感で調整

### 中期
- ローカルLLMで扱う問いの類型化
- CCテンプレートの自然淘汰
- モデル差による挙動の観測

### 長期（条件付き）
- より大きなローカルLLM環境が手に入った場合、
  小説支援への再適用を再検討
- Web検索・外部接続が可能になった場合、
  役割分担の再設計を行う

---

## 6. 未確定点・留保

- ローカルLLMの最適なモデルサイズの見極め
- CCの最適な保存頻度
- 「問い生成」をどこまで人間側で担うか
- ローカルLLMのガードレール設計余地

---

## 7. 再開ヒント

次に進むときは：

- CCログを数日分まとめて眺める
- 「保存されすぎ／足りなさ」を感覚で確認
- 運用が自然に回っているかを最優先で評価する

機能を足す前に、
**疲れていないか**を基準に判断する。
